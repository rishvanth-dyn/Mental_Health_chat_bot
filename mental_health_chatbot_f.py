# -*- coding: utf-8 -*-
"""Mental_health_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mHKmmYh0hzYvqFaNKHg4flOZq6x6x2GY
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
import random
from nltk.stem import WordNetLemmatizer
import json
import pickle
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD
from sklearn.preprocessing import LabelEncoder

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Tokenizing and lemmatizing
words = []
classes = []
documents = []
ignore_words = ['?', '!']

# Load intents data
data_file = open('intents.json').read()
intents = json.loads(data_file)

# Process intents
for intent in intents['intents']:
    for pattern in intent['patterns']:
        # Tokenize each word
        w = nltk.word_tokenize(pattern)
        words.extend(w)
        # Add documents in the corpus
        documents.append((w, intent['tag']))
        # Add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

# Lemmatize and lower each word and remove duplicates
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))

# Sort classes
classes = sorted(list(set(classes)))

# Output statistics
print(len(documents), "documents")
print(len(classes), "classes", classes)
print(len(words), "unique lemmatized words", words)

# Save words and classes
pickle.dump(words, open('texts.pkl', 'wb'))
pickle.dump(classes, open('labels.pkl', 'wb'))

from google.colab import drive
drive.mount('/content/drive')

# Create training data
training = []
output_empty = [0] * len(classes)

# Create training set
for doc in documents:
    # Create the bag of words
    bag = [1 if w in doc[0] else 0 for w in words]

    # Output is a binary array corresponding to the intent
    output_row = output_empty[:]
    output_row[classes.index(doc[1])] = 1

    # Print lengths for debugging
    print(f"Bag length: {len(bag)}, Output row length: {len(output_row)}")

    # Ensure that bag and output_row have correct shapes
    if len(bag) == len(words) and len(output_row) == len(classes):
        training.append([bag, output_row])
    else:
        print("Mismatch in lengths: bag -", len(bag), "output_row -", len(output_row))

# Shuffle and convert to NumPy array
random.shuffle(training)

# Reshape training data to have consistent shapes
# Use a list comprehension to extract the bag and output_row for each sample
train_x = np.array([x[0] for x in training])
train_y = np.array([x[1] for x in training])

print("Training data created")

# Create model - 3 layers
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(classes), activation='softmax'))

# Compile model
# Use learning_rate instead of lr
sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Fitting and saving the model
hist = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)
model.save('model.h5')
print("Model created")

!pip install flask flask-ngrok

!pip install pyngrok

import nltk
nltk.download('popular')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import pickle
import numpy as np
import tensorflow as tf
from keras.models import load_model
import json
import random

# Load your model (make sure the model is available in the same directory or specify path)
model = load_model('model.h5')

intents = json.loads(open('intents.json').read())
words = pickle.load(open('texts.pkl','rb'))
classes = pickle.load(open('labels.pkl','rb'))

def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0]*len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print("found in bag: %s" % w)
    return np.array(bag)

def predict_class(sentence, model):
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})
    return return_list

def getResponse(ints, intents_json):
    if ints:
        tag = ints[0]['intent']
        list_of_intents = intents_json['intents']
        for i in list_of_intents:
            if i['tag'] == tag:
                result = random.choice(i['responses'])
                break
        return result
    else:
        return "Sorry, I didn't understand that."

def chatbot_response(msg):
    res = getResponse(predict_class(msg, model), intents)
    return res

from flask import Flask, render_template, request
from flask_ngrok import run_with_ngrok
from pyngrok import ngrok

app = Flask(__name__, static_folder='static')

# Ngrok authentication token (make sure to set it properly)
ngrok.set_auth_token('2oi2citdQ49WW4cecn5POhcfBrk_64xfxeUrghnUs2WzvqCaU')

# Open a tunnel on the default port 5000
public_url = ngrok.connect(5000)
# Print the public URL
print(f" * ngrok tunnel \"{public_url}\" -> \"http://127.0.0.1:5000\"")

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/get")
def get_bot_response():
    userText = request.args.get('msg')
    print("get_bot_response:- " + userText)

    # Directly get the response in English without language detection
    chatbot_response_text = chatbot_response(userText)

    return chatbot_response_text

# Run the Flask app
if __name__ == "__main__":
    app.run(debug=True, use_reloader=False)

"""from google.colab import drive
drive.mount('/content/drive')
"""